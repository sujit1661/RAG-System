# ğŸ“˜ RAG-sync â€” PDF Retrieval-Augmented Generation System

RAG-sync is a simple end-to-end **Retrieval-Augmented Generation (RAG)** pipeline that takes PDF documents, converts them into embeddings, stores them in a local vector database, and retrieves the most relevant chunks to answer user queries using an AI model.

---

## ğŸ”„ How RAG Works (Quick Overview)

RAG has **two phases**:

### 1ï¸âƒ£ Indexing Phase (Data Injection)
- **Load PDFs**  
- **Chunk text** to maintain context  
- **Generate embeddings** using Sentence-Transformers (`all-MiniLM-L6-v2`)  
- **Store vectors** in ChromaDB for fast similarity search  

### 2ï¸âƒ£ Retrieving Phase (User Query â†’ Answer)
- Convert query â†’ embedding  
- Search similar chunks in vector DB  
- Select top relevant context  
- Provide context + question to an LLM  
- Generate grounded, accurate answers  

---

## ğŸ—„ï¸ Vector Database Used
**ChromaDB** â€” lightweight, local, fast, ideal for learning and small projects.

Other popular options: Pinecone, Qdrant, Weaviate, Milvus, FAISS.

---

## âš™ï¸ LangChain Usage
LangChain simplifies:
- Loading & splitting documents  
- Creating embeddings  
- Connecting to vector DB  
- Running retrieval pipelines  

---

## ğŸš€ How to Run

### Install dependencies:
pip install langchain chromadb sentence-transformers pypdf

### Run Indexing:
python indexing.py

### Run Retrieval:
python retriving.py
---
## ğŸ“Œ Note
`local_chroma_db/` is ignored in Git because it contains heavy vector files.

---

## ğŸ‘¤ Author
Sujit Sadalage

## ğŸ“ Project Structure

RAG-sync/
â”‚
â”œâ”€â”€ Data_Injection and Data_Retrieval/
â”‚ â”œâ”€â”€ indexing.py # Convert PDF â†’ chunks â†’ embeddings â†’ DB
â”‚ â”œâ”€â”€ retriving.py # Query search + context retrieval
â”‚ â””â”€â”€ local_chroma_db/ # Auto-generated vector DB (ignored)
â”‚
â”œâ”€â”€ blackbook.pdf
â””â”€â”€ README.md


